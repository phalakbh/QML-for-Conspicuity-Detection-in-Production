{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2- Variational Classifier\n",
    "\n",
    "In this notebook, we follow the tutorial on Variational classifiers using Pennylane([here](https://pennylane.ai/qml/demos/tutorial_variational_classifier/)) and explain what we have learnt, through comments within each code block as well as a brief summary in the end of the notebook.\n",
    "\n",
    "The first example shows that a variational circuit can\n",
    "be optimized to emulate the parity function\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f: x \\in \\{0,1\\}^{\\otimes n} \\rightarrow y =\n",
    "\\begin{cases} 1 \\text{  if uneven number of 1's in } x \\\\ 0\n",
    "\\text{ else}. \\end{cases}\n",
    "\\end{aligned}$$\n",
    "\n",
    "It demonstrates how to encode binary inputs into the initial state of\n",
    "the variational circuit, which is simply a computational basis state\n",
    "(*basis encoding*).\n",
    "\n",
    "The second example shows how to encode real vectors as amplitude vectors\n",
    "into quantum states (*amplitude encoding*) and how to train a\n",
    "variational circuit to recognize the first two classes of flowers in the\n",
    "Iris dataset.\n",
    "\n",
    "__Contributors:__ Nandan Patel, Phalak Bhatnagar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE 1: Fitting Parity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the quantum device with 4 qubits\n",
    "dev = qml.device(\"default.qubit\", wires=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(layer_weights):\n",
    "    for wire in range(4):\n",
    "        qml.Rot(*layer_weights[wire], wires=wire)\n",
    "\n",
    "    for wires in ([0, 1], [1, 2], [2, 3], [3, 0]):\n",
    "        qml.CNOT(wires)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Variational Quantum Circuit\n",
    "# A variational quantum circuit consists of layers of parameterized rotation gates(Rx(theta)) and CNOT gates, theta is determined by weights.\n",
    "# In the following code, we apply CNOT gates between qubits 0&1, 1&2, 2&3, 0&3\n",
    "def layer(layer_weights):\n",
    "    for wire in range(4):\n",
    "        qml.Rot(*layer_weights[wire], wires=wire)\n",
    "\n",
    "    for wires in ([0, 1], [1, 2], [2, 3], [3, 0]):\n",
    "        qml.CNOT(wires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Basis State encoding to encode the input values into our qubits,\n",
    "# For eg. 0101->|0101‚ü©\n",
    "def state_preparation(x):\n",
    "    qml.BasisState(x, wires=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function to prepare the state preparation circuit, it applies the quantum circuit layers with the given weights\n",
    "# Outputs an expectation value of the Z operator on the first qubit\n",
    "@qml.qnode(dev)\n",
    "def circuit(weights, x):\n",
    "    state_preparation(x)\n",
    "\n",
    "    for layer_weights in weights:\n",
    "        layer(layer_weights)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds a bias to the circuit to the output of the quantum circuit to make the model more effective\n",
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a cost function as the root of the difference between the actual labels and the predicted values\n",
    "def square_loss(labels, predictions):\n",
    "    # We use a call to qml.math.stack to allow subtracting the arrays directly\n",
    "    return np.mean((labels - qml.math.stack(predictions)) ** 2)\n",
    "# We also define the accuracy\n",
    "def accuracy(labels, predictions):\n",
    "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n",
    "    acc = acc / len(labels)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the cost function\n",
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the input data into x and y variables, x as input and y as predicted labels,\n",
    "# The dataset has y values ranging from 0 to 1, we shift it from -1 to 1\n",
    "data = np.loadtxt(r\"C:\\Users\\dpsso\\Desktop\\variational_classifier\\data\\parity_train.txt\", dtype=int)\n",
    "X = np.array(data[:, :-1])\n",
    "Y = np.array(data[:, -1])\n",
    "Y = Y * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "for x,y in zip(X, Y):\n",
    "    print(f\"x = {x}, y = {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize the weight values, and fix the number of layers and number of qubits\n",
    "np.random.seed(0)\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "print(\"Weights:\", weights_init)\n",
    "print(\"Bias: \", bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the NesterovMomentumOptimizer with bias as 0.5 and batch size as 5\n",
    "# This optimizer is used for enhanced efficiency and navigating the loss surface effectively\n",
    "opt = NesterovMomentumOptimizer(0.5)\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now execute our cicruit and run our optimizer to train our model\n",
    "# For each iteration, the cost and the accuracy of the model is calculated and the weights and bias is updated accordingly\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(100):\n",
    "\n",
    "    # Update the weights by one optimizer step, using only a limited batch of data\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    weights, bias = opt.step(cost, weights, bias, X=X_batch, Y=Y_batch)\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "\n",
    "    current_cost = cost(weights, bias, X, Y)\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training our model, we compare the predicted values and the actual y-labels for each set of x values\n",
    "# This step is important as it checks if our model, which gave excellent results in training, gives the same performance while using unseen test data\n",
    "data = np.loadtxt(r\"C:\\Users\\dpsso\\Desktop\\variational_classifier\\data\\parity_test.txt\", dtype=int)\n",
    "X_test = np.array(data[:, :-1])\n",
    "Y_test = np.array(data[:, -1])\n",
    "Y_test = Y_test * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "predictions_test = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "\n",
    "for x,y,p in zip(X_test, Y_test, predictions_test):\n",
    "    print(f\"x = {x}, y = {y}, pred={p}\")\n",
    "\n",
    "acc_test = accuracy(Y_test, predictions_test)\n",
    "print(\"Accuracy on unseen data:\", acc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get to know that the accuracy was 100% for this example.\n",
    "The goal of this example was used to accurately predict the output y as the output of the parity function.\n",
    "\n",
    "\n",
    "### EXAMPLE 2: Iris Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, instead of basis encoding, we use real 2-dimensional vectors\n",
    "#After adding \"latent dimensions(extra dimensions)\", we get the total no. of dimensions as 4, therefore we use 2 qubits as\n",
    "#2**2=4\n",
    "# In the last example, we used randomized weights as parameters for the rotation gates, here, we use 3 angles beta0, beta1 and beta2 as parameters\n",
    "# We get an array of 5 values derived from the 3 angle values which will be used as paramters\n",
    "def get_angles(x):\n",
    "    beta0 = 2 * np.arcsin(np.sqrt(x[1] ** 2) / np.sqrt(x[0] ** 2 + x[1] ** 2 + 1e-12))\n",
    "    beta1 = 2 * np.arcsin(np.sqrt(x[3] ** 2) / np.sqrt(x[2] ** 2 + x[3] ** 2 + 1e-12))\n",
    "    beta2 = 2 * np.arcsin(np.linalg.norm(x[2:]) / np.linalg.norm(x))\n",
    "\n",
    "    return np.array([beta2, -beta1 / 2, beta1 / 2, -beta0 / 2, beta0 / 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the state preparation, the sequence of CNOT gates and rotation gates, which take the above 5 generated values as paramters\n",
    "def state_preparation(a):\n",
    "    qml.RY(a[0], wires=0)\n",
    "\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[1], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[2], wires=1)\n",
    "\n",
    "    qml.PauliX(wires=0)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[3], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[4], wires=1)\n",
    "    qml.PauliX(wires=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, we take 4 input values and return 5 values derived from them, these are used as paramters for the rotation gates\n",
    "x = np.array([0.53896774, 0.79503606, 0.27826503, 0.0], requires_grad=False)\n",
    "ang = get_angles(x)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def test(angles):\n",
    "    state_preparation(angles)\n",
    "\n",
    "    return qml.state()\n",
    "\n",
    "\n",
    "state = test(ang)\n",
    "\n",
    "print(\"x               : \", np.round(x, 6))\n",
    "print(\"angles          : \", np.round(ang, 6))\n",
    "print(\"amplitude vector: \", np.round(np.real(state), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the layer and the cost function \n",
    "def layer(layer_weights):\n",
    "    for wire in range(2):\n",
    "        qml.Rot(*layer_weights[wire], wires=wire)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "\n",
    "\n",
    "def cost(weights, bias, X, Y):\n",
    "    predictions = variational_classifier(weights, bias, X.T)\n",
    "    return square_loss(Y, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "#Loading the IRIS dataset\n",
    "# Adding two extra dimensions as padding in order to make the dimensions similar to the state space dimensions of a 2-qubit vector\n",
    "# After normalizing the vector, we then generate the 5 parameters\n",
    "data = np.loadtxt(r\"C:\\Users\\dpsso\\Desktop\\variational_classifier\\data\\iris_classes1and2_scaled.txt\")\n",
    "X = data[:, 0:2]\n",
    "print(f\"First X sample (original)  : {X[0]}\")\n",
    "\n",
    "# pad the vectors to size 2^2=4 with constant values\n",
    "padding = np.ones((len(X), 2)) * 0.1\n",
    "X_pad = np.c_[X, padding]\n",
    "print(f\"First X sample (padded)    : {X_pad[0]}\")\n",
    "\n",
    "# normalize each input\n",
    "normalization = np.sqrt(np.sum(X_pad**2, -1))\n",
    "X_norm = (X_pad.T / normalization).T\n",
    "print(f\"First X sample (normalized): {X_norm[0]}\")\n",
    "\n",
    "# the angles for state preparation are the features\n",
    "features = np.array([get_angles(x) for x in X_norm], requires_grad=False)\n",
    "print(f\"First features sample      : {features[0]}\")\n",
    "\n",
    "Y = data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the graphs between different dimensions of the data(total dimensions are 4) to capture relationships between them\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0][Y == 1], X[:, 1][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n",
    "plt.scatter(X[:, 0][Y == -1], X[:, 1][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n",
    "plt.title(\"Original data\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "dim1 = 0\n",
    "dim2 = 1\n",
    "plt.scatter(X_norm[:, dim1][Y == 1], X_norm[:, dim2][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n",
    "plt.scatter(X_norm[:, dim1][Y == -1], X_norm[:, dim2][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n",
    "plt.title(f\"Padded and normalised data (dims {dim1} and {dim2})\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "dim1 = 0\n",
    "dim2 = 3\n",
    "plt.scatter(features[:, dim1][Y == 1], features[:, dim2][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n",
    "plt.scatter(features[:, dim1][Y == -1], features[:, dim2][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n",
    "plt.title(f\"Feature vectors (dims {dim1} and {dim2})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing(75% training and 25% testing)\n",
    "np.random.seed(0)\n",
    "num_data = len(Y)\n",
    "num_train = int(0.75 * num_data)\n",
    "index = np.random.permutation(range(num_data))\n",
    "feats_train = features[index[:num_train]]\n",
    "Y_train = Y[index[:num_train]]\n",
    "feats_val = features[index[num_train:]]\n",
    "Y_val = Y[index[num_train:]]\n",
    "\n",
    "# We need these later for plotting\n",
    "X_train = X[index[:num_train]]\n",
    "X_val = X[index[num_train:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the variables-no. of layers and no. of qubits along with weights and biases\n",
    "num_qubits = 2\n",
    "num_layers = 6\n",
    "\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train our model and minimize the cost\n",
    "opt = NesterovMomentumOptimizer(0.01)\n",
    "batch_size = 5\n",
    "\n",
    "# train the variational classifier\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(60):\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "    feats_train_batch = feats_train[batch_index]\n",
    "    Y_train_batch = Y_train[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, feats_train_batch, Y_train_batch)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = np.sign(variational_classifier(weights, bias, feats_train.T))\n",
    "    predictions_val = np.sign(variational_classifier(weights, bias, feats_val.T))\n",
    "\n",
    "    # Compute accuracy on train and validation set\n",
    "    acc_train = accuracy(Y_train, predictions_train)\n",
    "    acc_val = accuracy(Y_val, predictions_val)\n",
    "\n",
    "    if (it + 1) % 2 == 0:\n",
    "        _cost = cost(weights, bias, features, Y)\n",
    "        print(\n",
    "            f\"Iter: {it + 1:5d} | Cost: {_cost:0.7f} | \"\n",
    "            f\"Acc train: {acc_train:0.7f} | Acc validation: {acc_val:0.7f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data points of -1 class and 1 class and comparing their features according to a graph\n",
    "# The training and validation sets of data have been outlined to understand their relationships\n",
    "plt.figure()\n",
    "cm = plt.cm.RdBu\n",
    "\n",
    "# make data for decision regions\n",
    "xx, yy = np.meshgrid(np.linspace(0.0, 1.5, 30), np.linspace(0.0, 1.5, 30))\n",
    "X_grid = [np.array([x, y]) for x, y in zip(xx.flatten(), yy.flatten())]\n",
    "\n",
    "# preprocess grid points like data inputs above\n",
    "padding = 0.1 * np.ones((len(X_grid), 2))\n",
    "X_grid = np.c_[X_grid, padding]  # pad each input\n",
    "normalization = np.sqrt(np.sum(X_grid**2, -1))\n",
    "X_grid = (X_grid.T / normalization).T  # normalize each input\n",
    "features_grid = np.array([get_angles(x) for x in X_grid])  # angles are new features\n",
    "predictions_grid = variational_classifier(weights, bias, features_grid.T)\n",
    "Z = np.reshape(predictions_grid, xx.shape)\n",
    "\n",
    "# plot decision regions\n",
    "levels = np.arange(-1, 1.1, 0.1)\n",
    "cnt = plt.contourf(xx, yy, Z, levels=levels, cmap=cm, alpha=0.8, extend=\"both\")\n",
    "plt.contour(xx, yy, Z, levels=[0.0], colors=(\"black\",), linestyles=(\"--\",), linewidths=(0.8,))\n",
    "plt.colorbar(cnt, ticks=[-1, 0, 1])\n",
    "\n",
    "# plot data\n",
    "for color, label in zip([\"b\", \"r\"], [1, -1]):\n",
    "    plot_x = X_train[:, 0][Y_train == label]\n",
    "    plot_y = X_train[:, 1][Y_train == label]\n",
    "    plt.scatter(plot_x, plot_y, c=color, marker=\"o\", ec=\"k\", label=f\"class {label} train\")\n",
    "    plot_x = (X_val[:, 0][Y_val == label],)\n",
    "    plot_y = (X_val[:, 1][Y_val == label],)\n",
    "    plt.scatter(plot_x, plot_y, c=color, marker=\"^\", ec=\"k\", label=f\"class {label} validation\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned to build and train a quantum classifier using a hybrid quantum-classical approach. We began by understanding the theory behind variational quantum circuits and their role in quantum machine learning. We then moved on to implement the classifier with PennyLane and PyTorch, starting with data preparation, followed by constructing a variational circuit, and finally training the model. We also evaluated the classifier's performance on a simple dataset, gaining insights into the practical applications of quantum machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
