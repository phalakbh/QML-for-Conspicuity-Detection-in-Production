{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3- Quanvolutional Neural Networks\n",
    "\n",
    "In this notebook, we follow the tutorial on Quanvolutional Neural Networks using Pennylane([here](https://pennylane.ai/qml/demos/tutorial_quanvolution/)) and explain what we have learnt, through comments within each code block as well as a brief summary in the end of the notebook.\n",
    "\n",
    "__Contributors:__ Nandan Patel, Phalak Bhatnagar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quanvolutional Neural Networks\n",
    "\n",
    "The idea behind a classical CNN is that a kernel/filter is applied to an input image to extract its important features.\n",
    "The kernel is applied to different sub-images within the input image. The relevant features of the image are extracted and a new image is formed by collecting the features of these sub-images.\n",
    "\n",
    "\n",
    "Steps in a QNN:\n",
    " - The pixels of our sub-image are encoded in a variational circuit\n",
    " - A unitary U is then applied over the sub-image(it could be a pre-defined unitary, could also be generated randomly)\n",
    " - The system is then measured and expectation values are calculated\n",
    " - We map the expectation values to the pixels of the output which are then mapped onto a resulting image \n",
    " - Similarly, we can create a full output image using the calculated expectation values\n",
    " - We can perform further operations on the output image using classical/quantum layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates import RandomLayers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameter values\n",
    "n_epochs = 30   # Number of optimization epochs\n",
    "n_layers = 1    # Number of random layers\n",
    "n_train = 50    # Size of the train dataset\n",
    "n_test = 30     # Size of the test dataset\n",
    "\n",
    "SAVE_PATH = r\"C:\\Users\\dpsso\\Desktop\\QNN\"  # Data saving folder\n",
    "PREPROCESS = True           # If False, skip quantum processing and load data from SAVE_PATH\n",
    "np.random.seed(0)           # Seed for NumPy random number generator\n",
    "tf.random.set_seed(0)       # Seed for TensorFlow random number generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "mnist_dataset = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
    "\n",
    "# Reduce dataset size\n",
    "train_images = train_images[:n_train]\n",
    "train_labels = train_labels[:n_train]\n",
    "test_images = test_images[:n_test]\n",
    "test_labels = test_labels[:n_test]\n",
    "\n",
    "# Normalize pixel values within 0 and 1\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "\n",
    "# Add extra dimension for convolution channels\n",
    "train_images = np.array(train_images[..., tf.newaxis], requires_grad=False)\n",
    "test_images = np.array(test_images[..., tf.newaxis], requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the quantum circuit, this circuit is the unitary U that is then used as a kernel\n",
    "dev = qml.device(\"default.qubit\", wires=4)\n",
    "# Random circuit parameters\n",
    "rand_params = np.random.uniform(high=2 * np.pi, size=(n_layers, 4))\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(phi):\n",
    "    # Encoding of 4 classical input values becuase the sub-image is of size 2x2\n",
    "    for j in range(4):\n",
    "        qml.RY(np.pi * phi[j], wires=j)\n",
    "\n",
    "    # Random quantum circuit\n",
    "    RandomLayers(rand_params, wires=list(range(4)))\n",
    "\n",
    "    # Measurement producing 4 classical output values, these 4 values are then projected onto 4 pixels of the output image\n",
    "    return [qml.expval(qml.PauliZ(j)) for j in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The kernel is projected over the 4 squares of the input pixels and the output pixel values are assigned the expectation values\n",
    "def quanv(image):\n",
    "   \"\"\"Convolves the input image with many applications of the same quantum circuit.\"\"\"\n",
    "   out = np.zeros((14, 14, 4))\n",
    "\n",
    "   # Loop over the coordinates of the top-left pixel of 2X2 squares\n",
    "   for j in range(0, 28, 2):\n",
    "       for k in range(0, 28, 2):\n",
    "           # Process a squared 2x2 region of the image with a quantum circuit\n",
    "           q_results = circuit(\n",
    "               [\n",
    "                   image[j, k, 0],\n",
    "                   image[j, k + 1, 0],\n",
    "                   image[j + 1, k, 0],\n",
    "                   image[j + 1, k + 1, 0]\n",
    "               ]\n",
    "           )\n",
    "           # Assign expectation values to different channels of the output pixel (j/2, k/2)\n",
    "           for c in range(4):\n",
    "               out[j // 2, k // 2, c] = q_results[c]\n",
    "   return out\n",
    "\n",
    "\n",
    "# # We apply this the above part to perform pre-preprocessing as it is a one-time thing, it won't consume much resources\n",
    "# On the other hand, if we apply a quantum circuit to every step of image processing, it would be computationally expensive and inefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing images\n",
    "if PREPROCESS == True:\n",
    "    q_train_images = []\n",
    "    print(\"Quantum pre-processing of train images:\")\n",
    "    for idx, img in enumerate(train_images):\n",
    "        print(\"{}/{}        \".format(idx + 1, n_train), end=\"\\r\")\n",
    "        q_train_images.append(quanv(img))\n",
    "    q_train_images = np.asarray(q_train_images)\n",
    "\n",
    "    q_test_images = []\n",
    "    print(\"\\nQuantum pre-processing of test images:\")\n",
    "    for idx, img in enumerate(test_images):\n",
    "        print(\"{}/{}        \".format(idx + 1, n_test), end=\"\\r\")\n",
    "        q_test_images.append(quanv(img))\n",
    "    q_test_images = np.asarray(q_test_images)\n",
    "\n",
    "    # Save pre-processed images\n",
    "    np.save(SAVE_PATH + \"q_train_images.npy\", q_train_images)\n",
    "    np.save(SAVE_PATH + \"q_test_images.npy\", q_test_images)\n",
    "\n",
    "\n",
    "# Load pre-processed images\n",
    "q_train_images = np.load(SAVE_PATH + \"q_train_images.npy\")\n",
    "q_test_images = np.load(SAVE_PATH + \"q_test_images.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the convolution to the input image and getting 4 output channels, these output channels would then be mapped onto the output image pixels\n",
    "n_samples = 4\n",
    "n_channels = 4\n",
    "fig, axes = plt.subplots(1 + n_channels, n_samples, figsize=(10, 10))\n",
    "for k in range(n_samples):\n",
    "    axes[0, 0].set_ylabel(\"Input\")\n",
    "    if k != 0:\n",
    "        axes[0, k].yaxis.set_visible(False)\n",
    "    axes[0, k].imshow(train_images[k, :, :, 0], cmap=\"gray\")\n",
    "\n",
    "    # Plot all output channels\n",
    "    for c in range(n_channels):\n",
    "        axes[c + 1, 0].set_ylabel(\"Output [ch. {}]\".format(c))\n",
    "        if k != 0:\n",
    "            axes[c, k].yaxis.set_visible(False)\n",
    "        axes[c + 1, k].imshow(q_train_images[k, :, :, c], cmap=\"gray\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classical Image Processing\n",
    "## Now the quantum part is over, we take the pre-processed images and classify them into 10 different channels\n",
    "def MyModel():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model based on the pre-processed images from the quantum convolutional layer\n",
    "q_model = MyModel()\n",
    "\n",
    "q_history = q_model.fit(\n",
    "    q_train_images,\n",
    "    train_labels,\n",
    "    validation_data=(q_test_images, test_labels),\n",
    "    batch_size=4,\n",
    "    epochs=n_epochs,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In order to compare, we skip the quantum pre-processing part and directly train the model\n",
    "# Therefore, we compare classical vs quantum CNNs\n",
    "# Initializing the model with a classical instance\n",
    "c_model = MyModel()\n",
    "\n",
    "c_history = c_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    batch_size=4,\n",
    "    epochs=n_epochs,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 9))\n",
    "\n",
    "ax1.plot(q_history.history[\"val_accuracy\"], \"-ob\", label=\"With quantum layer\")\n",
    "ax1.plot(c_history.history[\"val_accuracy\"], \"-og\", label=\"Without quantum layer\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(q_history.history[\"val_loss\"], \"-ob\", label=\"With quantum layer\")\n",
    "ax2.plot(c_history.history[\"val_loss\"], \"-og\", label=\"Without quantum layer\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_ylim(top=2.5)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # We see that even though the accuracy of the classical model is high, due to the absence of pre-processing, it costs more to evaluate and compute.\n",
    "# Therefore, QCNNs lower the cost while maintaing the same level of accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "We learned how to combine quantum circuits with classical convolutional neural networks. We explored the concept of a quanvolutional layer, where quantum circuits process input data to extract features. We implemented this using PennyLane and TensorFlow, preparing the dataset, designing the quanvolutional layer, and integrating it into a neural network. We also learned to train and evaluate the model on a classical machine learning task, demonstrating the potential of quantum-enhanced feature extraction."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
